{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c58838d",
   "metadata": {},
   "source": [
    "# 1. Optimization\n",
    "\n",
    "$$\\max_\\theta \\ell (\\theta), \\label{eqn}$$\n",
    "\n",
    "$$\\ell(\\theta) = \\sum_{i=1}^m \\left\\{-\\log (1+\\exp\\{-\\theta^T x^i\\}) + (y^i-1) \\theta^T x^i\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c1465",
   "metadata": {},
   "source": [
    "###### 1. Show step-by-step mathematical derivation for the gradient of the cost function l(θ) in (1)\n",
    "\n",
    "$$\\frac{\\partial \\mathscr{l}(\\theta)}{\\partial \\theta} = \\sum_{i=1}^{n}(y_i x_i - x_i - (\\frac{e^{\\theta^T x_i}}{1 + e^{\\theta^T x_i}})x_i)$$\n",
    "\n",
    "$$= \\sum_{i=1}^{n}(y - \\frac{1}{1 + e^{\\theta^{T}x}})x_i$$\n",
    "\n",
    "\n",
    "###### 2. Write a pseudo-code for performing gradient descent to find the optimizer θ∗. This is essentially what the training procedure does.\n",
    "\n",
    "To maximize the likelihood of the parameters ($\\theta$), use gradient ascent\n",
    "\n",
    "GD: \n",
    "\n",
    "initialize $\\theta$\n",
    "\n",
    "While ${\\theta^{new} - \\theta^{old} > \\epsilon}$\n",
    "    \n",
    "$$\\theta_i^{new} = \\theta_i^{old} + \\lambda \\sum_{i=1}^{n}(y - \\frac{1}{1 + e^{\\theta^{T}x}})x_i$$\n",
    "\n",
    "\n",
    "##### 3. Write the pseudo-code for performing the stochastic gradient descent algorithm to solve the training of logistic regression problem (1). Please explain the difference between gradient descent and stochastic gradient descent for training logistic regression.\n",
    "\n",
    "SGD: \n",
    "\n",
    "- initialize $\\theta$\n",
    "\n",
    "- While ${\\theta^{new} - \\theta^{old} > \\epsilon}$\n",
    "\n",
    "- sample $K$ from the data\n",
    "\n",
    "- for $k$ in $K$ do\n",
    "    \n",
    "$$\\theta_k^{new} = \\theta_i^{old} + \\lambda \\sum_{i=1}^{K}(y - \\frac{1}{1 + e^{\\theta^{T}x}})x_i$$\n",
    "\n",
    "Stochastic Gradient Decent is only different in that you there is a sampled selection of data points the algorthim is ran on at a time. To do the gradient on the entire dataset can be computationally expensive, and the subset allows it to converge more quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4c15e",
   "metadata": {},
   "source": [
    "# 2. Comparing Classifiers\n",
    "\n",
    "### 2.1 Divorce classification/prediction\n",
    "\n",
    "##### a.) Report testing accuracy for each of the three classifiers. Comment on their perfromance: which performs the best and make a guess why they perform the best in this setting. \n",
    "\n",
    "The data was broken into train/test splits with 20% of the data reserved for testing. Each of the models performed the **exact** same. The random seed selected produced 97% accuracy for Naive Bayes, Logisitic Regression, and KNN. This speaks to the dataset and one of the most important asepcts to machine learning, understanding the data. \n",
    "\n",
    "The model used should be a function of the data analysis. Each classifier has it's own strengths and weaknesses. For this dataset, none of the weaknesses were exposed, which most likely means that the data is easily seperable and almost any solid classification algorithm will work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca6261",
   "metadata": {},
   "source": [
    "##### b.) Plot the data points and decision boundary of each classifier in the two-dimensional space. Comment on the difference between the decision boundary for the three classifiers. Please clearly represent the data points with different labels using different colors.\n",
    "\n",
    "Using PCA, I was able to transform the training set into a 2 dimensional space. Using the fitted training data, the test data can be transformed as well, which makes it easier to visualize the data points and decision boundary. \n",
    "\n",
    "As noted above, the classifiers all perfomed exceptionally well. The random seed would affect if it was 97 or 100% accurate. Shown below, the data is very clear that the divorce target variable has clear separation in the predictor variables. You could effectively look at making a rule based prediction provided the data. There only appears to be 1 single outlier in the dataset. \n",
    "\n",
    "The decision boundaries for Naive Bayes and Logistic regression are mostly linear and linear respectively. The KNN model has a non-linear decision boundary.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d0930",
   "metadata": {},
   "source": [
    "\n",
    "![](img/q2_part1_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6eb59e",
   "metadata": {},
   "source": [
    "### Handwritten digit classification\n",
    "\n",
    "##### a.) Report confusion matrix, precision, recall, and F-1 score for each of the classifiers. For precision, recall, and F-1 score of each classifier, we will need to report these for each of the digits. So you can create a table for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0fb6a",
   "metadata": {},
   "source": [
    "The dataset contains handwritten digits 0-9, and a multiclass classification problem. Outputted below is the confusion matrix, along with metrics: percision, recall, and f-1 score. These metrics are able to define model performance better than raw accuracy scores. \n",
    "\n",
    "The confusion matrix identifies how well each of the combinations of classifications is predicted. Prior knowledge of handwritten digits is intuitively noticed with things such as higher misclassification with 1 and 7, since they do look fairly similar. Same thing with (3, 8) and (4, 9). Ideally we want as many on the diagonal as possible, but all of the models performed well and pass the eye test with the confusion matrices. \n",
    "\n",
    "Accuracy score doesn't represent the model fully. If there are a low amount of target variables in a certain category, the accuracy could still be relatively high even though the model outputted 0 instances of that class though observed.\n",
    "\n",
    "Precision is defined as $\\frac{TP}{TP + FP}$ which identifies the proportion that were accurately predicted. \n",
    "\n",
    "Recall (sensitivity) is defined as $\\frac{TP}{TP + FN}$ and represents the proportion of actual positives identified correctly. \n",
    "\n",
    "Together these metrics can provide key insight on actual model performance. Both of them work in harmony to produce the F1-Score, which is a measure of the accuracy with provided weights to precision or recall. \n",
    "\n",
    "These metrics work on binary classification variables, which is why they were assesed for each digit in the dataset. The aggregated scores are taken to give a wholistic view of the model.\n",
    "\n",
    "###### Perfomance\n",
    "\n",
    "The Logistic Regression and Linear SVM performed the worst out of the group. The data has non-linear properties that would make sense to cause those to perform worse. \n",
    "\n",
    "The best performing model was the Kernel SVM. The kernel trick really helped classify with the non-linear properties. I think it is important to note, that there was minimal model tuning with KNN and Neural Network. Neural Network would have probably performed the best if given the resources to tune with. It is typically the best for image classification and learning highly non-linear datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a2036",
   "metadata": {},
   "source": [
    "![](img/knn.png)\n",
    "\n",
    "---\n",
    "![](img/logistic_regression.png)\n",
    "\n",
    "---\n",
    "![](img/linear_svm.png)\n",
    "\n",
    "---\n",
    "![](img/kernel_svm.png)\n",
    "\n",
    "---\n",
    "![](img/nerual_network.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da32d741",
   "metadata": {},
   "source": [
    "# Naive Bayes for spam filtering\n",
    "\n",
    "##### 1. Calculate class prior P(y = 0) and P(y = 1) from the training data, where y = 0 corresponds to spam messages, and y = 1 corresponds to non-spam messages. Note that these class prior essentially corresponds to the frequency of each class in the training sample. Write down the feature vectors for each spam and non-spam messages.\n",
    "\n",
    "\n",
    "- **V = {secret, offer, low, price, valued, customer, today, dollar, million, sports, is, for, play, healthy, pizza}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533341c",
   "metadata": {},
   "source": [
    "$P(y=0) = 3/7$  \n",
    "$P(y=1) = 4/7$\n",
    "\n",
    "###### Spam\n",
    "million dollar offer = $[0,1,0,0,0,0,0,1,1,0,0,0,0,0,0]$  \n",
    "secret offer today =   $[1,1,0,0,0,0,1,0,0,0,0,0,0,0,0]$  \n",
    "secret is secret =     $[2,0,0,0,0,0,0,0,0,0,1,0,0,0,0]$\n",
    "\n",
    "###### Not Spam\n",
    "low price for valued customers = $[0,0,1,1,1,1,0,0,0,0,0,1,0,0,0]$  \n",
    "play secret sports today =       $[1,0,0,0,0,0,1,0,0,1,0,0,1,0,0]$  \n",
    "sports is healthy =              $[0,0,0,0,0,0,0,0,0,1,1,0,0,1,0]$  \n",
    "low price pizza =                $[0,0,1,1,0,0,0,0,0,0,0,0,0,0,1]$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b56f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de80a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686ef0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bbcd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65ac9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13774678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a1f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6dc68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a27c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b74294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9cf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
