{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2c1f41",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fedc4",
   "metadata": {},
   "source": [
    "## 1.) Concept Questions\n",
    "\n",
    "#### 1.) Whatâ€™s the main difference between supervised and unsupervised learning?\n",
    "\n",
    "The main difference between supervised and unsupervised learning is having the labels of the training data prior to modeling. In supervised learning, you would be classifying a test data set after training on data that was already labeled or classified. Unsupervised learning, such as k-means, would be learning the clusters or classifications from unlabeled data. Unsupervised learning also can call for adding subjectivity or domain knowledge into the analysis to make sense of the output. \n",
    "\n",
    "\n",
    "#### 2.) Will different initializations for k-means lead to different results?\n",
    "\n",
    "Yes, due to the fact that it is a non-convex optimization problem. This could lead to multiple local solutions and we would only end up in one of them. Depending on the starting point, if there are multiple local solutions you could end up at different solutions based on initializaiton. This also means that it is not guarenteed to find the global minimal optimization solutions. \n",
    "\n",
    "\n",
    "#### 3.) Give a short proof (can be in words but using correct logic) why k-means algorithm will converge in finite number of iterations.\n",
    "\n",
    "\n",
    "Since there are a fixed number of data points, there are only a finite number of combinations to calculate the centroids. The algorithm will try to reduce a certain metric, and you can continue to decrease until it is not able to decrease any further. This could take many iterations, but eventually it will always converge. \n",
    "\n",
    "\n",
    "#### 4.) What is the main difference between k-means and generalized k-means algorithm? Explain how the choice of the similarity/dissimilarity/distance will impact the result.\n",
    "\n",
    "The differnce everything to do with the similarity measure. K-means uses Euclidean distance, where the generalized k-means algorithm can use any similarity measure. Replacing the Euclidean distance with a general definition can allow it to be a convex optimization problem. This also allows customization for different types of datasets where Euclidian is not the best solution for the objective.\n",
    "\n",
    "\n",
    "#### 5.) Write down the graph Laplacian matrix and find the eigenvectors associated with the zero eigen- value. Explain how do you find out the number of disconnected clusters in graph and identify these disconnected clusters using these eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce61ca4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -1, -1,  0,  0],\n",
       "       [-1,  2, -1,  0,  0],\n",
       "       [-1, -1,  2,  0,  0],\n",
       "       [ 0,  0,  0,  1, -1],\n",
       "       [ 0,  0,  0, -1,  1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0]\n",
    "    ])\n",
    "\n",
    "D = np.array([\n",
    "    [2, 0, 0, 0, 0],\n",
    "    [0, 2, 0, 0, 0],\n",
    "    [0, 0, 2, 0, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "L = D - A\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e33530fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, V = np.linalg.eig(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395224b3",
   "metadata": {},
   "source": [
    "##### Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22518356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81649658, -0.57735027,  0.30959441,  0.        ,  0.        ],\n",
       "       [-0.40824829, -0.57735027, -0.80910101,  0.        ,  0.        ],\n",
       "       [-0.40824829, -0.57735027,  0.49950661,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.70710678,  0.70710678],\n",
       "       [ 0.        ,  0.        ,  0.        , -0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4affa456",
   "metadata": {},
   "source": [
    "##### Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09a2b8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.00000000e+00, -3.77809194e-16,  3.00000000e+00,  2.00000000e+00,\n",
       "        0.00000000e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920721b4",
   "metadata": {},
   "source": [
    "##### Identifying the clusters\n",
    "To find the clusters from the eigenvectors, the eigenvectors with eigenvalue 0 contain cluster assignment information.The rows that have similar eigenvectors are from similar communities, and clustered together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12639028",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.) Image compression using clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db3b6f",
   "metadata": {},
   "source": [
    "#### 1.) Euclidean\n",
    "\n",
    "Utilizing the Kmeans cluster algorithm, I was able to create image compressions by clustering the individual pixels of the image. Each image begins with randomized centorids bassed on the $k$ clusters passed to the method. It then iterates assigning each row to the closest centorid based on the $L2$ distance. The centroids are then updated by taking the average of each cluster group. This continues until convergence where the centroids no longer change (stopping when the centorids are less than 0.001 in difference than the previous iteration). \n",
    "\n",
    "For the analysis, each image was ran 5 times per cluster, on $k = [2, 4, 6, 8, 16]$.\n",
    "\n",
    "As the number of clusters increased, the iterations and time it took to converge also increased. For the Football image, lower $k$ completed in an average of 11 to 18 seconds, where higher $k$ finished around 40 seconds. The variance was much higher as you increase $k$. Depending on the randomized centroids, the solution could converge quickly, or it could take quite a while relative to other runs due to poor starting points. It is possible to select the exact sample pixel (not point, but same pixel elsewhere in the image) and that would cause an error with the algorthim. To handle this, if the number of unique initilized centroids did not equal $k$, they were re-initilized until that held true.\n",
    "\n",
    "The Georgia Tech image had interesting results. For $k = 2$, it converged quickly with minimal variance. For $k = 4$, it still converged quickly in an average of 22 iterations, but the variance was 133. I don't believe it is a property internal to the image, but it does show that centroid selection to begin can have a large affect on the model run time. \n",
    "\n",
    "I then ran the algorithm on two other images, of my cat and child. The image of the baby was notable because the variance  was much higher for $k=8$ than that of $k=16$. They did not seem to be as complex as the provided images, converging much faster than the other 2. \n",
    "\n",
    "Outputs of the images below. The higher the $k$, the more pixelated the image. It depends on the tradeoff you are looking for as to which works best for you. If this was to input thousands of images into a model, it might be better to use a lower $k$, and capture the rough outlines.\n",
    "\n",
    "$k = 6$\n",
    "![football image with 6 clusters and euclidean distance](img/football_6_cluster_euclidean.png)\n",
    "\n",
    "$k = 16$\n",
    "![georgia tech image with 16 clusters and euclidean distance](img/GeorgiaTech_16_cluster_euclidean.png)\n",
    "\n",
    "$k = 6$\n",
    "![baby with 6 clusters and euclidean distance](img/IMG_1401_6_cluster_euclidean.png)\n",
    "\n",
    "$k = 4$\n",
    "![cat with 4 clusters and euclidean distance](img/cat_4_euclidean.png)\n",
    "\n",
    "\n",
    "#### 2.) Manhattan \n",
    "\n",
    "The next part of the analysis was replicating the algorithm, except this time using the Manhattan distance. This would be taking the $L1$ norm, where the distance is calculated by taking the sum of length of the $Y$ axis direction and $X$ axis direction. Manhattan is the nickname due to it's similarity of a taxi driving through blocks in Manhattan. Centroids were updated using the median values for each iteration. \n",
    "\n",
    "By using the Manhattan distance, there was an increase in the time it took to converge relative to the Euclidean distance for the Football, Baby, and Cat images. However, the Georgia Tech image converged quicker with Manhatten.\n",
    "\n",
    "$k = 16$\n",
    "![](img/football_16_cluster_cityblock.png)\n",
    "\n",
    "$k = 4$\n",
    "![](img/GeorgiaTech_4_cluster_cityblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8de141",
   "metadata": {},
   "source": [
    "## 3.) Political blogs dataset\n",
    "\n",
    "#### 1.) \n",
    "\n",
    "The data provided contained all possible nodes, as well as connected edges. To perform spectral clustering, a Laplacian matrix needed to be derived from a degree and adjacency matrix. The degree matrix is a diagonal matrix, where the diagonal is the size of the node (number of unique edges connected to it). The rest of the values are 0. Almost the opposite, the adjacency matrix is a binary matrix where a value signifies that there was a connection between the two nodes.\n",
    "\n",
    "Spectral clustering was done on values of $k = [2, 5, 10, 20]$. The assigned value was given based off of the majority in that cluster. \n",
    "\n",
    "$k = 2$\n",
    "- cluster 1: 46.7%\n",
    "- cluster 2: 49%\n",
    "\n",
    "$k = 5$\n",
    "- cluster 1: 11.4%\n",
    "- cluster 2: 1.7%\n",
    "- cluster 3: 14.7%\n",
    "- cluster 4: 15%\n",
    "- cluster 5: 6.5%\n",
    "\n",
    "$k = 10$\n",
    "- cluster 1: 7%\n",
    "- cluster 2: 6.9%\n",
    "- cluster 3: 13%\n",
    "- cluster 4: 18.4%\n",
    "- cluster 5: 13.7%\n",
    "- cluster 6: 19.5%\n",
    "- cluster 7: 3.1%\n",
    "- cluster 8: 1%\n",
    "- cluster 9: 12.6%\n",
    "- cluster 10: 3.3%\n",
    "\n",
    "\n",
    "$k = 20$\n",
    "- cluster 1: 0%\n",
    "- cluster 2: 8.3%\n",
    "- cluster 3: 15%\n",
    "- cluster 4: 5.1%\n",
    "- cluster 5: 3.2%\n",
    "- cluster 6: 13.6%\n",
    "- cluster 7: 2.5%\n",
    "- cluster 8: 3.2%\n",
    "- cluster 9: 1.8%\n",
    "- cluster 10: 10%\n",
    "- cluster 11: 10.6%\n",
    "- cluster 12: 6.1%\n",
    "- cluster 13: 1.5%\n",
    "- cluster 14: 31%\n",
    "- cluster 15: 4.3%\n",
    "- cluster 16: 12.6%%\n",
    "- cluster 17: 20%\n",
    "- cluster 18: 12%\n",
    "- cluster 19: 1.9%\n",
    "- cluster 20: 50%\n",
    "\n",
    "\n",
    "#### 2.) Tunning $k$ values\n",
    "\n",
    "To tune for the best $k$ parameter value, I iterated through the spectral_graph function on a list of values of $k$ from 2 to 40, incremented by 2. The metric used was the total observations missclassified divided by the total observations, to create an overall missclassification rate. \n",
    "\n",
    "When $k$ is small, the missclassifciation rate was high. As $k$ increased, the missclassifcation rate decreased, untill the range of 18 - 22, after that there was a steady increase in the missclassifcation rate. This is typical in other machine learning models, where the bias and variance tradeoff sees diminishing returns as overfitting takes place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a5c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
