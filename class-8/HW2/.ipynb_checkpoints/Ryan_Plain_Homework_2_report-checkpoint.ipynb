{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514968cd",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## 1. Conceptual Questions\n",
    "\n",
    "#### 1. Please prove the first principle component direction $v$ corresponds to the largest eigenvector of the sample covariance matrix.\n",
    "\n",
    "$$v = \\arg \\max_{w: ||w|| \\le 1} \\frac{1}{m}\\sum_{i=1}^{m}(w^Tx^i - w^T\\mu)^2$$\n",
    "\n",
    "\n",
    "The covariance matrix for matrix $X$ of $m$ features is $M = E[(X - \\mu)(X - \\mu)^T]$. \n",
    "\n",
    "PCA finds the vector (direction) where the variance is maximized. The first principal component will, due to the maximization objective, will correspond to the largest eigenvector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9449f",
   "metadata": {},
   "source": [
    "#### 2. What is the relationship between SVD and eigendecomposition? Please explain this mathematically, and touch on why this is relevant for PCA. \n",
    "\n",
    "A factorization of a rel matrix. $C = U$\n",
    "\n",
    "Eigendecomposition relies on square matricies (typically symetrical), where as SVD will exist for any rectangular or square matrix. SVD is more general in that since. \n",
    "\n",
    "SVD is a product of 3 matricies:\n",
    "$$M = U\\Sigma V^T$$\n",
    "\n",
    "- $U \\in \\mathbb{R}^{n x m}$ -- left singular vectors (orthonormal)\n",
    "- $\\Sigma \\in \\mathbb{R}^{nxm}$ -- singular values\n",
    "- $V \\in \\mathbb{R}^{mxm}$ -- Right singular vectors (orthonormal)\n",
    "\n",
    "The eigenvectors of $C := MM^T$ is the $U$ left singular vectors\n",
    "\n",
    "The eigenvalues of $C$ is $\\sigma^{2}_i$ (squared singular values of $M$)\n",
    "\n",
    "To use it in PCA, the eigenvectors of $U$ and $V$ can be used with the number of columns as the number of principal components from the eigenvalues in the $\\Sigma$ matrix. To understand the amount of variance explained, you can take the sum of the principal components used,  divided by the sum of all principal components. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda910e1",
   "metadata": {},
   "source": [
    "#### 3. Explain the three key ideas of ISOMAP (for manifold learning and non-linear dimensionality reduction.)\n",
    "\n",
    "Geodesic distance to capture distance between the points. \n",
    "\n",
    "- A weighted nearest neighbors method is applied\n",
    "- Find the shortest path distance matrix D between each pairs of points\n",
    "- Find low dimensional representation that preservce \n",
    "Produce low dimensional representation which preserves \"walking distance\" over the manifold. \n",
    "\n",
    "Shortest distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546638ba",
   "metadata": {},
   "source": [
    "## 2. Eigenfaces and simple face recognition\n",
    "\n",
    "### a.)\n",
    "\n",
    "Subject 01 Eigenfaces\n",
    "\n",
    "1. ![](img/face01_1_vector.png) \n",
    "2. ![](img/face01_2_vector.png)\n",
    "3. ![](img/face01_3_vector.png)\n",
    "4. ![](img/face01_4_vector.png)\n",
    "5. ![](img/face01_5_vector.png)\n",
    "6. ![](img/face01_6_vector.png)\n",
    "\n",
    "Subject 02 Eigenfaces\n",
    "\n",
    "1. ![](img/face02_1_vector.png)\n",
    "2. ![](img/face02_2_vector.png)\n",
    "3. ![](img/face02_3_vector.png)\n",
    "4. ![](img/face02_4_vector.png)\n",
    "5. ![](img/face02_5_vector.png)\n",
    "6. ![](img/face02_6_vector.png)\n",
    "\n",
    "\n",
    "Each image was vectorized, reduced by a factor of 4, and added to a dataset that was $M = (10, 4880)$ The first 6 eigenfaces are represented by individual eigenvectors extracted from SVD on the covariance matrix. \n",
    "\n",
    "The first pictures are the most recognizable, because the first eigenvalue and eigenvector used captures the most variance in the dataset. As we progress through the eigenvectors, different shapes and features from unqique poses are captured. Eigenvectors such as those $\\ge 4$ contain more characteristicss that are represented of the individual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2ee81",
   "metadata": {},
   "source": [
    "### b.)\n",
    "\n",
    "To do face recognition, we need to use the top eigenvector from PCA to test against the new image. I used the SVD approach again, in which the eigenvectors are already sorted by the largest eigenvalues. The new test image was centered on the mean of the original training images. \n",
    "\n",
    "Taking the squared $L2$ norm of the projection residual, we get the following values for the eigenvector and test subject:\n",
    "\n",
    "Subject 01  \n",
    "- $s_{11}$:  49,933,494  \n",
    "- $s_{12}$:  113,948,550  \n",
    "  \n",
    "Subject 02  \n",
    "- $s_{21}$:  199,761,842  \n",
    "- $s_{22}$:  181,971,406  \n",
    "\n",
    "\n",
    "In both test cases, the residuals of the incorrect test subject are larger than the correct test subject. This can be used to classify which subject the test images belong to.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22194e69",
   "metadata": {},
   "source": [
    "## 4. PCA: Food consumption in European Countries\n",
    "\n",
    "#### a.)\n",
    "\n",
    "Using SVD, I was able to recreate the matrix using the first 2 principle components and respective eigenvectors. The scatter plot below shows the relationship of the first 2 principle components. \n",
    "\n",
    "![](img/PCA_2.png)\n",
    "\n",
    "The countries that are clustered together show similar food characteristics. Doing this analysis makes it easier to view high dimensional data in a low dimensional setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
