{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0fefe4",
   "metadata": {},
   "source": [
    "# 1. Conceptual Questions\n",
    "\n",
    "#### 1.) Based on the outline given in the lecture, show mathemtically that the maximum likelihood estimate (MLE) for Gaussian mean and variance parameters are given by \n",
    "$$\n",
    "\\hat \\mu = \\frac 1 m \\sum_{i=1}^m x^i, \\quad \\hat \\sigma^2 = \\frac 1 m \\sum_{i=1}^m (x^i - \\hat \\mu)^2\n",
    "$$\n",
    "\n",
    "Note: For this derivation, you will also need to show that these estimates for $\\mu$ and $\\sigma$ are **maximum**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636317e7",
   "metadata": {},
   "source": [
    "Gaussian distribution has two sets of parameters $(\\mu, \\sigma)$\n",
    "\n",
    "The likeihood of one data point is:\n",
    "$$p(x^i | \\mu, \\sigma) \\propto exp(-\\frac{1}{2 \\sigma^2}(x^i - \\mu)^2 )$$\n",
    "\n",
    "The paramter estimates are:\n",
    "\n",
    "$$\\hat \\mu_{MLE} = \\frac 1 m \\sum_{i=1}^m x^i, \\quad \\hat \\sigma^2_{MLE} = \\frac 1 m \\sum_{i=1}^m (x^i - \\hat \\mu)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305570c",
   "metadata": {},
   "source": [
    "The objective function is log likelihood:\n",
    "\n",
    "$$l(\\mu, \\sigma; D) = log \\prod_{i=1}^{m} \\frac{1}{(2 \\pi)^{\\frac{1}{2}} \\sigma} exp(-\\frac{1}{2\\sigma^2}(x^i - \\mu)^2)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ff8af",
   "metadata": {},
   "source": [
    "$$= - \\frac{m}{2} log( 2\\pi) - \\frac{m}{2} log (\\sigma^2) - \\sum_{i=1}^{m}\\frac{(x^i - \\mu)^2}{2\\sigma^2}$$\n",
    "\n",
    "\n",
    "Maximize $l(\\mu, \\sigma; D)$ with respect to $\\mu, \\sigma$\n",
    "\n",
    "Take derivates w.r.t. $\\mu, \\boldsymbol{\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e94c0e",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial l }{\\partial \\mu} = 0$$\n",
    "$$ \\frac{\\partial l }{\\partial \\sigma^2} = 0$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8639e",
   "metadata": {},
   "source": [
    "$$= - \\frac{m}{2} log( 2\\pi) - \\frac{m}{2} log (\\sigma^2) - \\sum_{i=1}^{m}\\frac{(x^i - \\mu)^2}{2\\sigma^2}$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial l }{\\partial \\mu} = \\frac{1}{\\sigma^2}[(x_1 + ... + x_n) - n\\mu]$$\n",
    "\n",
    "$$0 = \\frac{1}{\\sigma^2}[(x_1 + ... + x_n) - n\\mu]$$\n",
    "\n",
    "$$\\mu = \\frac{\\sum_{i=1}^n x_i}{n}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial l }{\\partial \\sigma^2} = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}[(x_1 - \\mu)^2) + ... + (x_n - \\mu)^2]$$\n",
    "\n",
    "$$0 = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}[(x_1 - \\mu)^2) + ... + (x_n - \\mu)^2]$$\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d45600",
   "metadata": {},
   "source": [
    "#### 2.) Please compare the pros and cons of KDE as opposed to histograms, and give at least one advantage and disadavantage of each.\n",
    "\n",
    "- KDE\n",
    "    - Pros \n",
    "        - smooth density where boxes are located in histogram\n",
    "        - reduces noise of arbitrary bin size (advantage over histogram)\n",
    "        - Better in high dimensional data (advantage to histogram)\n",
    "    - Cons \n",
    "        - Have to hold all the data in memory \n",
    "        - Parameters increase with $m$, more expensive computation \n",
    "            - Histogram would have the advantage depending on the size of bins, and memory requirement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba31ec3",
   "metadata": {},
   "source": [
    "#### 3.) For the EM algorithm for GMM, please show how to use Bayes rule to derive $\\tau_k^i$ in closed-form expression. \n",
    "\n",
    "let $\\theta = (\\pi_k, \\mu_k, \\Sigma_k), k = 1, ..., K)$\n",
    "\n",
    "Maximize: $\\theta^{t+1} = argmax_\\theta f(\\theta)$\n",
    "\n",
    "    - Prior\n",
    "$$p(z)$$\n",
    "\n",
    "    - Likelihood\n",
    "$$p(x |z) = N(x| \\mu_z, \\Sigma_z)$$\n",
    "\n",
    "    - Posterior\n",
    "$$p(z|x) = \\frac{\\pi_z N(X|\\mu_z, \\Sigma_z)}{\\Sigma_z, \\pi_z, N(X|\\mu_{z'}, \\Sigma_{z'}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\tau_k^i = p(z^i = k|x^i, \\theta^t) = \\frac{p(x^i|z^i=k)p(z^i=k)}{\\Sigma_{k' = 1..K}p(z^i = k', x^i)}$$\n",
    "\n",
    "$$ = \\frac{\\pi_z N(X|\\mu_z, \\Sigma_z)}{\\Sigma_z, \\pi_z, N(X|\\mu_{z'}, \\Sigma_{z'})}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b4546",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Density estimation: Psychological experiments\n",
    "\n",
    "#### a.) Form the 1-dimensional histogram and KDE to estimate the distributions of amygdala and acc, respectively. For this question, you can ignore the variable orienta- tion. Decide on a suitable number of bins so you can see the shape of the distribution clearly. Set an appropriate kernel bandwidth h > 0\n",
    "\n",
    "To get the Histogram and KDE, manually checking different parameters for bin size and bandwidth were used. I wasn't able to use the rule of thumb Silverman method since this did not match a normal distribution. \n",
    "\n",
    "The bandwith selected of 0.3, gives enough balance to capture modes without over interpolating the data an blurring out these points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c006307",
   "metadata": {},
   "source": [
    "![](img/plot_2_a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea0cf9",
   "metadata": {},
   "source": [
    "#### b.) Form 2-dimensional histogram for the pairs of variables (amygdala, acc). Decide on a suitable number of bins so you can see the shape of the distribution clearly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5096b76",
   "metadata": {},
   "source": [
    "The number of bins selected was **8**. This was able to best identify where the the modal points were jointly distributed. It also demonstrates how much data is needed to even fill out a bin size as small as that, limiting the effectiveness of multi-dimensional histograms. Lowering the bins further diminishes seeing how spread out the data is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313f032",
   "metadata": {},
   "source": [
    "![](img/plot_2_b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf221b",
   "metadata": {},
   "source": [
    "#### c.) Use kernel-density-estimation (KDE) to estimate the 2-dimensional density function of (amygdala, acc) (this means for this question, you can ignore the variable orientation). Set an appropriate kernel bandwidth h > 0.\n",
    "\n",
    "At all the levels of $h$ checked manually, the joint distribution appears to be unimodal. Additionally, you can  see that there are several outliers, visuzalized as points that are outside the scope of the KDE plot. \n",
    "\n",
    "Looking at this visually, I would determine that there is a strong relationship between the variables and they are not independent. Next steps would be to test statistically how strong their dependence is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb98a6",
   "metadata": {},
   "source": [
    "![](img/plot_2_c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbef73",
   "metadata": {},
   "source": [
    "#### d.)\n",
    "\n",
    "![](img/plot_2_d_1.png)\n",
    "![](img/plot_2_d_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9828f",
   "metadata": {},
   "source": [
    "##### Conditional Sample Means\n",
    "\n",
    "\n",
    "|   orientation |    amygdala |         acc |\n",
    "|--------------:|------------:|------------:|\n",
    "|             2 |  0.0190615  | -0.0147692  |\n",
    "|             3 |  0.0005875  |  0.00167083 |\n",
    "|             4 | -0.00471951 |  0.00130976 |\n",
    "|             5 | -0.00569167 |  0.00814167 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb7394",
   "metadata": {},
   "source": [
    "We can conclude that the distributions of the individual variables is conditional on the orientation. Orientation 4 appears to be the most common, and with 3 make up most of the values. This logically makes sence due to the fact that the orientations are more mild political opinions, and would (hopefully) contain more samples. Although some skewness, they appear to be more Gaussian. \n",
    "\n",
    "Orientation 2 and 5 are more extreme political views. This is seen clearly with `acc` and orientation 2, where there is a strong right skewed modal around -0.025. \n",
    "\n",
    "The table shows empirically the difference in sample means conditioned on orientation. The scale of `amygdala` is positive and negative values split between 3 and 4. `acc` has the only negative mean with orietation 2, and the difference between 2 and 5 is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceed8e7",
   "metadata": {},
   "source": [
    "#### 3.) Joint Conditional Probability\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530876b",
   "metadata": {},
   "source": [
    "![](img/plot_2_e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da9971",
   "metadata": {},
   "source": [
    "From the data presented, it can be infered that the distribution of political orientation is conditioned on the brain regions `amygdala` and `acc`. This though is just an inference based on the data provided. It is possible that bringing in more data points shows that the predictor variables are correlated strongly with another more powerful predictor. That information would be missing in this analysis, and attributing the signal to the wrong feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428057b",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 3. Implementing EM for MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f425d",
   "metadata": {},
   "source": [
    "#### a.)\n",
    "\n",
    "#### b.)\n",
    "\n",
    "Plotting the log liklihood vs the number of iterations. The algorithm appears to have converged quickly, and it is good to see that it remained stable until meeting the threshold. \n",
    "\n",
    "![](img/part_2_b.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
