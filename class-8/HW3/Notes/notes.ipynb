{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d674642b",
   "metadata": {},
   "source": [
    "# Non Parametric Models  \n",
    "\n",
    "Parametric\n",
    "- Gaussian distribution is determenined by mean and covariance matrix parameters\n",
    "\n",
    "\n",
    "Non Parametric\n",
    "- No pre specified shape of the distribution\n",
    "    - KDE\n",
    "    - Histogram\n",
    "\n",
    "KDE can capture where you have bi-modal data, vs Gaussian will only return one mode\n",
    "\n",
    "\n",
    "### Parametric Models\n",
    "\n",
    "Can be described by a fixed number of parameters\n",
    "\n",
    "- Discrete\n",
    "    - Bernoulli distribution\n",
    "    - One parameter, $\\theta \\in [0, 1]$, which generate a family of models\n",
    "- Continuous\n",
    "    - Gaussian distribution\n",
    "        - $(\\mu, \\sigma)$\n",
    "- Probabilistic graph models\n",
    "    - Probability relationship between variables\n",
    "    - Dependence of these random variables\n",
    "\n",
    "### Non Parametric Models\n",
    "\n",
    "- Smooth density pdf\n",
    "\n",
    "- Histogram\n",
    "- KDE\n",
    "\n",
    "Non parametric does NOT mean there are no parameters\n",
    "- Can not be described by a fixed number of parameters\n",
    "    - Multivariate gaussian has fixed params\n",
    "    - Non-param model, we donâ€™t want to fix the parameters\n",
    "        - The degree of freedom are not fixed\n",
    "        - Models are quite flexible\n",
    "\n",
    "\n",
    "## MLE\n",
    "\n",
    "Simple and has good statistical properties. \n",
    "\n",
    "Data $D = {x^1, x^2, .. x^n}$ for iid from some unknown distribution $P^{*}(x)$\n",
    "\n",
    "- iid means drawn from the same distribution as well\n",
    "\n",
    "Want to fit the data with a model $P(x|\\theta)$\n",
    "\n",
    "$\\hat\\theta = argmax_{\\theta} log P(D|\\theta)$\n",
    "$= argmax_{\\theta} log \\prod_{i=1}^{m} P(x^i|\\theta)$\n",
    "\n",
    "You want to find the probabilkity of your data being maxed. \n",
    "The product of their distibution \n",
    "\n",
    "\n",
    "use the log, so the product becomes the sum of the log.\n",
    "- maximize the sum of a bunch of variables instead of maximizing the products\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0312182",
   "metadata": {},
   "source": [
    "Example\n",
    "\n",
    "- Estimate the probability $\\theta$ of landing in heads for a biased coin\n",
    "\n",
    "Given a sequence of $m$ *i.i.d.* flips\n",
    "\n",
    "$$D = {x^1, x^2, ..., x^m} = {1, 0,1, ..., 0}, x^i \\in {0, 1}$$\n",
    "\n",
    "### Both are the same ways to write\n",
    "\n",
    "Model: $P(x| \\theta) = \\theta^x (1 - \\theta)^{1 - x}$\n",
    "\n",
    "- Compact expression\n",
    "\n",
    "$P(x|\\theta)=  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 - \\theta, x=0 \\\\\n",
    "      \\theta, x = 1 \\\\\n",
    "\\end{array} \n",
    "\\right.  $\n",
    "\n",
    "- Piecewise\n",
    "\n",
    "\n",
    "\n",
    "- Likelihood of a single observation $x_i$\n",
    "\n",
    "$P(x^i | \\theta) = \\theta^{x^i} = \\theta^{x^i}(1 - \\theta)^{1 - x^i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05138",
   "metadata": {},
   "source": [
    "$\\theta$ is probability of getting a HEADS\n",
    "\n",
    "### MLE for biased coin\n",
    "\n",
    "- objective function: log likelihood\n",
    "\n",
    "- log is the sum over all the samples and the likelihood for each individual example\n",
    "\n",
    "- Property of the log to bring down $x^i$ to sum of $x^i * log$\n",
    "\n",
    "$l(\\theta; D) = log P(D | \\theta) = log \\theta^{n_h}(1 - \\theta)^{n_t} = n_h log \\theta + (m - n_h) log(1 - \\theta)$\n",
    "\n",
    "$n_h$: number of heads, $n_t$: number of tails\n",
    "\n",
    "- maximize $l(\\theta; D)$ w.r.t. $\\theta$\n",
    "\n",
    "- Take derivatives w.r.t. $\\theta$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\theta} = \\frac{n_h}{\\theta} - \\frac{(m - n_h)}{1 - \\theta}$  \n",
    "\n",
    "$\\Rightarrow \\theta = \\frac{n_h}{m} or \\hat\\theta_{MLE} = \\frac{1}{m}\\Sigma_i x^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487f95d",
   "metadata": {},
   "source": [
    "derivate of l and set to zero to maximize \n",
    "\n",
    "## Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a9e95",
   "metadata": {},
   "source": [
    "Gaussian distribution in R\n",
    "\n",
    "$p(X | \\mu, \\sigma = \\frac{1}{(2\\pi)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c3096",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "- Histograms\n",
    "    - Too many bins in high dimensional data\n",
    "    - Most bins will be empty, grows exponentaly fast\n",
    "    - We will not have a meaningful estimate\n",
    "    \n",
    "    - Output depends on where you put the bins: estimates are **noisy*\n",
    "    - arbitrary bin size produce different histograms\n",
    "    \n",
    "    \n",
    "- KDE\n",
    "    - approximate density from histogram box shaped functions\n",
    "    - smooth density where boxes are located\n",
    "    - Place one smoothing kernel centered at the data point\n",
    "        - After you place all the kernels, sum them together to interpolate the data points\n",
    "        \n",
    "        \n",
    "   - Kernel choices\n",
    "       - gaussian\n",
    "       - tophat (not popular)\n",
    "       - epanechnikov\n",
    "       - exponential\n",
    "       - linear\n",
    "       - cosine\n",
    "       \n",
    "   - kernel bandwith\n",
    "       - too large: too much interpolation and blue out everything\n",
    "       - to small captures too many modes\n",
    "       \n",
    "       \n",
    "How to choose kernel bandwidth?\n",
    "\n",
    "If you are using guassian, $h = 1.06 \\hat\\sigma m^{-1/5}$\n",
    "\n",
    "OR \n",
    "\n",
    "A better approach is cross validate\n",
    "\n",
    "- randomly split the data\n",
    "- obtain kernel density estimate using one set\n",
    "- measure the likelihood of the second set\n",
    "- repeat over many splits and average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c351444",
   "metadata": {},
   "source": [
    "Drawback to KDE\n",
    "- In order to represent density function, you have to keep all the data in memory\n",
    "- If you have a lot of data, it is better to summarize the data\n",
    "- Most expensive computation\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e39f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd27eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89394f37",
   "metadata": {},
   "source": [
    "# EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d3908",
   "metadata": {},
   "source": [
    "$\\tau_1^i$ is the probability of likelihood that the $i^{th}$ data point comes from this gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f212dbc",
   "metadata": {},
   "source": [
    "Latent variable - hidden variable to randomly choose a mixture component. \n",
    "- after, sample the actual value of $x^i$ from a gaussian dist $N(x|u_z^i, \\Sigma_z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e592d9a",
   "metadata": {},
   "source": [
    "We don't know the latent vector\n",
    "- impute missing information by taking the expectation with respect to unkown latent factros\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d32a52",
   "metadata": {},
   "source": [
    "## Expecation Maximization\n",
    "\n",
    "1. Compute E: take expecation over posterior conditioned on date: forms a lower bound\n",
    "2. Maximize: $\\theta^{t+1} = argmax_\\theta f(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1f72",
   "metadata": {},
   "source": [
    "1. Start with initilization of theta\n",
    "2. use that to find lower bound \n",
    "3. maximize \n",
    "4. Estimate posterior\n",
    "4. Improve by maximizing lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bc990",
   "metadata": {},
   "source": [
    "$$P(z|x) = \\frac{P(x|z)P(z)}{P(x)}$$\n",
    "\n",
    "$$\\frac{P(x,z)}{\\Sigma_z , P(x, z')}$$\n",
    "\n",
    "$$Posterior = \\frac{likelihood * prior}{normalization constant}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61df90a",
   "metadata": {},
   "source": [
    "$$Prior = p(z) = \\pi_z$$\n",
    "\n",
    "- Margianl distribution of $Z$\n",
    "\n",
    "$$Likelihood: p(x|z) = N(x|\\mu_z, \\Sigma_z$$\n",
    "\n",
    "- Told what z is, what should be distribution of x\n",
    "\n",
    "$$Posterior: p(z|x) = \\frac{\\pi_z N(X|\\mu_z, \\Sigma_z)}{\\Sigma_z, \\pi_z, N(X|\\mu_{z'}, \\Sigma_{z'}}$$\n",
    "\n",
    "- Probability dist of z given x, trying to make a guess if I know x best guess about z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a05123",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "$$p(x, z) = p(x|z)p(z) = p(z|x)p(x)$$\n",
    "\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e70ea",
   "metadata": {},
   "source": [
    "$p(x)$ is the marginal distribution, ensuring that this will sum up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd41ca9",
   "metadata": {},
   "source": [
    "# E-step: find posterior\n",
    "\n",
    "$$q(z^1, z^2, .., z^m) = \\prod_{i=1}^{m} p(z^i | x^i, \\theta^t)$$\n",
    "\n",
    "for each data point $x^i$ compute $p(z^i = k|x^i, \\theta^t)$ for each $k$\n",
    "\n",
    "$$\\tau_k^i = p(z^i = k|x^i, \\theta^t) = \\frac{p(x^i|z^i=k)p(z^i=k)}{\\Sigma_{k' = 1..K}p(z^i = k', x^i)}$$\n",
    "\n",
    "$$ = \\frac{\\pi_z N(X|\\mu_z, \\Sigma_z)}{\\Sigma_z, \\pi_z, N(X|\\mu_{z'}, \\Sigma_{z'}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a78144",
   "metadata": {},
   "source": [
    "# Estep: compute expectation\n",
    "\n",
    "$$(\\theta) := E_{q(z^1, z^2, .., z^m)} [log \\prod_{i=1}^{m} p(x^i, z^i | \\theta)] = \\sum_{i=1}^{m} E_{p(z^i|x^i, \\theta^t)}[log p(x^i, z^i|\\theta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e7317",
   "metadata": {},
   "source": [
    "Expand log of Gaussian density $log N(x^i | \\mu_{z^i}, \\Sigma_{z^i}$\n",
    "\n",
    "$$ f(\\theta) = \\sum_{i=1}^{m} E_{p(z^i|x^i, \\theta^t)} [log\\pi_{z^i} - \\frac{1}{2}(x^i - \\mu_{z^i}^T \\)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df39c04",
   "metadata": {},
   "source": [
    "$\\prod_{i=1]^{m} p(x^i, z^i | \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df839371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
